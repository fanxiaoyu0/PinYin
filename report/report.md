<h2><center>拼音输入法报告</center></h2>

<center><h6>2019013273 范逍宇</h6></center>

### 项目配置

+ ##### 语言和版本配置

  + python:使用python作为编程语言。

  + 版本：python-3.7

  + 调用库文件：

    ```python
    import os
    import math
    import pickle
    import time
    import sys
    ```
  
+ 启动方式

  + 在src文件夹目录下，使用命令行输入命令，将作为输入的txt文件名和保存输出结果的txt文件名作为参数即可，默认保存在data文件夹下。

    命令格式举例：
    $$
    python \quad字三元.py \quad input.txt \quad output.txt
    $$
    
  + 对于“字四元.py”，如果运行时电脑的可用内存小于8GB，可能存在长时间无响应的情况，而其他的两个模型都能在正常情况下能很快地运行。

  + 如果最终检测正确率时只能使用一个版本，我想标记“字三元.py”为最终版本。

+ 文件结构：

  + data（存放测试用例）
    + input.txt：用于测试的输入文件
    + output.txt：用于保存输出结果的文件
    + test（个人手工整理的数据集，从2016-2的新闻中获取的短句随机抽取1000条）
      + IN.txt（上述短句在翻译网站翻译成拼音https://www.qqxiuzi.cn/zh/pinyin/）
      + OUT-2.txt（使用基于字的二元模型得到的结果）
      + OUT-3.txt（使用基于字的三元模型得到的结果）
      + OUT-4.txt（使用基于字的四元模型得到的结果）
      + Answer.txt（存有原句作为标准答案）
  + src（源代码）
    + 字二元.py（实现基于字的二元模型）
    + 字三元.py（实现基于字的三元模型）
    + 字四元.py（实现基于字的四元模型）
  + 模型（存放训练得到的模型）
    + 字二元
      + 字频
      + 二元词频
    + 字三元
      + 字频
      + 二元词频
      + 三元词频
    + 字四元
      + 字频
      + 二元词频
      + 三元词频
      + 四元词频

### 算法的基本思路和实现过程

+ 数据集选择与划分
  + 仅使用本次作业提供的语料集进行训练（即新浪新闻语料）
  + 将2016-2的新闻作为测试集，其余语料作为训练集
+ 数据清洗
  + 编码格式转换：将所给的语料的编码格式全部转换为utf-8，便于对中文字符进行操作。
  + 提取出汉字部分：将所给的语料先切分成字符，然后将其中的汉字部分提取出来，这里不考虑数字，英文，标点符号对语义的影响，将所有非汉字字符全部删去。
  + 将新闻分成短句：考虑到一行内的新闻其实是由许多短句组成的，一个句子的最后一个字与下一个句子的第一个字不是相邻的。所以将连续的一段汉字认定为一个短句，其中不能包含其他任何字符。（这里的汉字仅指给定的拼音汉字对照表中出现的汉字，其他的汉字也当做无关字符处理）
  + 筛选掉过短的句子：对于只有一两个字的短句，可以认为其中包含的汉字之间关系的信息很少，而且可能对正确的语义关系产生干扰，所以将短句中这样的句子删去（实验也证明了确实对提升正确率有帮助）。
+ 字频表和条件频率矩阵
  + 字频表：对于清洗好的数据，统计每个字出现的频率，使用字典进行存储。
  + 二元词、三元词、四元词频率张量：统计二元词，三元词，四元词出现的频率，存入二维、三维、四维张量中（使用多维字典实现），用于在之后计算条件概率。
+ 算法
  + 频率作为概率：在语料集比较大的情况下，可以将字，词出现的频率作为概率。
  + 马尔科夫模型：使用贝叶斯公式计算条件概率，结合隐马尔科夫假设构建语言模型，用于预测一个汉字字符串是句子的概率。
  + viterbi算法：使用viterbi算法对一个汉字字符串是句子的概率进行估计，使用动态规划算法代替搜索，大大加快了程序运行速度。
  + 用链表结构输出结果：在viterbi算法中，每个拼音对应的汉字字符节点记录自己是从上一层的哪个状态转移过来的，在最后一层进行比较找出最后一层的最优节点后，依次向前寻找前一个状态，从而将整个字符串输出出来。
  + 取对数：使用取对数的方法将乘法运算转化为加法运算，大大减少了程序的运行时间。
  + 平滑操作：对于要找的二元词或多元词从未在语料集中出现的情况，设置一个很小的平滑参数，用这个平滑参数乘以这个字或多元词的概率作为结果，这样不仅能保证取对数有意义，也能通过调节平滑参数进一步提升模型的准确率。
  + 字典存储稀疏矩阵：使用多维字典存储二元词，三元词，四元词的频率张量，大大压缩了存储空间，特别是在三元词的处理上十分有效。

### 实验效果

注：以下准确率均基于上文提到过的从2016-2的新闻中随机抽取1000条短句做成的数据集。

+ 集于字的二元模型

  + 训练集：除2016-2之外的所给语料集

  + 字准确率：88.987 %

  + 句准确率：49.1 %

  + 好的例子：

    ```html
    jing ji jian she he wen hua jian she tu chu le shi ba da jing shen de zhong yao xing
    经济建设和文化建设突出了十八大精神的重要性
    tai nan shi duo chu lou fang zai jin tian ling chen de di zhen guo hou qing xie
    台南市多处楼房在今天凌晨的地震过后倾斜
    zhong guo jing ji zeng su fang huan de wai yi xiao ying bei kua da
    中国经济增速放缓的外溢效应被夸大
    bu shao chuang ye qi ye tong guo jin ru xi fen shi chang shi tu kai tuo geng da de shi chang kong jian
    不少创业企业通过进入细分市场试图开拓更大的市场空间
    ```

    分析：以上句子的翻译均与答案完全相符，这里句子能被精准的翻译出的一个很重要原因是其中大多数词为二元词，且这些二元词在语料库中出现频率不低。

  + 坏的例子

    + 多音字：

      ```
      拼音：zai zhong ji wei dui qi li an diao cha qi jian qi zhu dong jiao dai le qi shou hui fan zui shi shi
      翻译：在中纪委对其立案调查期间起主动较大了其受贿犯罪事实
      答案：在中纪委对其立案调查期间起主动交代了其受贿犯罪事实
      ```

      分析：这里将"交代"翻译成"较大"，一是因为本次所给语料没有对应拼音，训练时无法对多音字进行区分，二是因为"较大"出现的频率高于"交代"出现的频率，三是因为二元模型无法对上下文语义进行理解。

    + 人名、地名：

      ```
      ji ge yue qian ta wu yi zhong fa xian wang shu hong lao ren zai tu shu guan zuo hua
      几个月前他无疑中发现王姝泓老人在图书馆作画
      几个月前他无疑中发现王姝泓老人在图书馆作画
    ```
    
    分析：对于人名、地名，即使目前市面上的输入法也难以进行准确预测，大多数为根据用户以往的输入历史进行推测，目前没有很好的解决办法。
    
  + 同音字：
    
      ```
      heng bin shi wei wan shan jie dai wai guo ren de huan jing he zhen xing shi nei guan guang
      横滨市未完善接待外国人的环境和振兴市内观光
    ```
    

+ 基于字的三元模型

  + 训练集：除2016-2之外的所给语料集

  + 字准确率：94.05 % 

  + 句准确率：69.4 %

  + 其他测试集：

    值得一提的是，该模型在其他测试集上也有不错的效果，在群中给出的往届同学征集的数据集上，其字准确率为89.25 %，句准确率为57.635 %，对于除少数偏难怪的句子外，对大多数正常的句子都给出了完全准确甚至超出预期的结果。
  
    + 好的例子：

      ```
    ji qi xue xi ji qi ying yong
      机器学习及其应用
    chun jiang chao shui lian hai ping
      春江潮水连海平
      yi ge zi ren wei qian li bu fan de ren tong guo jian ku zhuo jue de nu li
      一个自认为潜力不凡的人通过艰苦卓绝的努力
    ben ci pu cha huo dong you zhu yu bang zhu tong xue men zou chu xin li wu qu
      本次普查活动有助于帮助同学们走出心理误区
    gei a yi dao yi bei ka bu qi nuo
      给阿姨到一杯卡布奇诺
    ```
  
    其中最后一个句子虽然不完全正确，但使用三元模型成功预测出了四元词“卡布奇诺”还是令人感到惊喜的。
  
    + 坏的例子：
  
      ```
      拼音：san bing yi hao lu ben wei zhun bei jiu xu
      翻译：伞柄尾号路本为准备就绪
      答案：伞兵一号卢本伟准备就绪
      拼音：jin fang xiao tou ba chuang qiao
      翻译：谨防小头爸闯桥
      答案：谨防小偷把窗撬
      拼音：ta men shi ren shang ren er wo shi lao shu ren jian bu de guang de lao shu ren
      翻译：他们是人伤人而我是老熟人间不得光的老熟人
    答案：他们是人上人而我是老鼠人见不得光的老鼠人
      拼音：da yan she de gong ji li lai dao le yi wan san qian dian
    翻译：大堰社的攻击力来到了一万三千点
      答案：大岩蛇的攻击力来到了一万三千点
    ```
  
      这些语句的翻译错误一方面是因为语句来源于较新的网络流行语，而训练集为2016年的新闻，训练集与测试集的语料存在较大偏差，另一方面，这种语言模型也很难兼顾到上下文的关系，不能根据语义给出合理的预测。
  
+ 基于字的四元模型

  + 训练集：仅2016-4的语料，因为四元词的频率矩阵维数较高，而python对字典的空间设置导致在运行时的字典占用空间大小远远大于保存成pkl文件后的字典占用空间大小（字典保存成pkl文件仅400M，但在运行时占用空间约为10G），而我的电脑内存仅为8GB，如果对更大的语料集进行训练，训练过程中当python项目占用内存过多时windows系统会对字典进行压缩，导致无法顺利保存成pkl文件，下一步可能对这个问题进行进一步探究。

  + 字准确率：91.478 %

  + 句准确率：60.3 %

  + 好的例子：

    ```
    来皆是厄尔尼诺
    中共中央关于制定国民经济和社会发展第十三个五年规划的建议
    毒物检验报告及现场调查所取得的资料进行分析研究和甄别
    ```


错误原因分析：

+ 数据集不够大，很多字与字没有同时出现过，但它们之间确实有语义上的联系。
+ 仅适用基于字的二，三，四元模型，不能分析到字的上下文的语义关系，丧失了很多连贯的语义信息。
+ 一些训练集中未出现的人名、地名很难被预测出来。

### 参数选择与性能分析

注：以下准确率均基于上文提到过的从2016-2的新闻中随机抽取1000条短句做成的数据集。

+ 参数选择：
  + 二元字：

    + 停用字：对于不常出现的字，可以认为其为生僻字，在进行预测是直接将其排除在外，设置参数a，去除所有语料集中出现次数小于a的汉字，以下是a对准确率的影响：（此时b/c=$10^{10}$）

      | a    | 字准确率 | 句准确率 |
      | ---- | -------- | -------- |
      | 0    | 88.987 % | 49.1 %   |
      | 1    | 88.987 % | 49.1 %   |
      | 10   | 88.987 % | 49.1 %   |
      | 100  | 88.978 % | 49.1 %   |
      | 1000 | 88.88 %  | 48.8 %   |

      分析：可以看出，停用字对准确率的影响不大，但是可以减少运算次数，提升程序 运行效率，基于经验，我们将a设置为10，可以在不降低准确率的前提下尽可能多地进行剪枝，在之后的两个模型中，也默认采用这个参数，不再讨论。

    + 平滑参数：在条件概率公式中引入平滑参数，即：
      $$
      P(w_i|w_{i-1}):替换为\rightarrow b\times P(w_i|w_{i-1})+c\times P(w_i)
      $$
      其中参数b，c本身的绝对大小影响不大，因为在viterbi算法每层的比较过程中，每个节点的b，c相同，所以b，c本身的绝对大小不起作用，真正对准确率产生影响的是b与c的相对大小关系，具体的影响为，提升b/c可以有效提升句准确率的大小，这是因为b代表词的频率在最终预测中的占比，占比越大，句子越具有整体性，最终选择的b/c的为：$10^{10}$。

  + 三元字：

    + 基于字的四元模型的调参方法与上面类似，平滑操作为：
      $$
      P(w_i|w_{i-3}w_{i-2}w_{i-1}):替换为\\ 
      \rightarrow b\times P(w_i|w_{i-2}w_{i-1})+c\times P(w_i|w_{i-1})+ d\times P(w_i)
      $$
      最终选择的参数为：（b，c，d）=（$9\times 10^{12}$，$2\times 10^{12}$，1）（表示的是相对于d的相对大小）

  + 四元字：

    + 平滑操作为：
      $$
      P(w_i|w_{i-3}w_{i-2}w_{i-1}):替换为\\ 
      \rightarrow b\times P(w_i|w_{i-3}w_{i-2}w_{i-1})+c\times P(w_i|w_{i-2}w_{i-1})+ d\times P(w_i|w_{i-1})+e\times P(w_i)
      $$
      最终选择的参数为：（b，c，d，e）=（$2\times 10^{10}$，$9\times 10^{12}$，$2\times 10^{12}$，1）（表示的是相对于d的相对大小）

+ 时间复杂度：在复杂度方面，本模型使用的是viterbi算法，预测过程的复杂度为$O(kn^2)$，其中k为句子长度，n为每个拼音对应的汉字的平均字数（即每个节点的平均分支数）。这种算法的效率远远高于搜索算法，使得程序能在较快的时间内输出结果，也便于根据反馈结果对参数进行调节，具体来说，在我自己的电脑上，预测测试集的1000条句子，二元字模型仅需约4s，三元字模型仅需约8s，四元字模型需要约60s（其中很大一部分时间花在打开和载入四元词的模型文件）。

### 收获

+ 更加熟练地使用python，特别是字符串的处理和文件的加载和保存操作运用得更加自如，同时也掌握了一些调试技巧，例如对中间过程生成的文件进行保存可以在下次运行时直接加载使用，大大减少了重复的工作。
+ 深入了解了语言模型的原理，对隐马尔科夫模型和贝叶斯公式有了更深入地理解，在实际的应用过程中感受到了语言模型在自然语言处理的过程中的巨大作用。
+ 深入学习了viterbi算法，经过实际的运行效率对比，感受到动态规划远远高于暴力搜索的效率。
+ 认识到模型参数与训练语料的关联性很强，在新闻语料上训练的模型应用到生活词汇的预测上可能会效果很差，例如在同学们征集的数据集上模型的准确率大大下降。
+ 感受到了自主探索的过程的魅力，我在之前没有接触过人工智能方面的算法，在此次试验中有稍许吃力，最近两个星期几乎每天晚上都有固定两个小时的时间对进行模型构建和参数优化，虽然无法与优秀的同学的模型相比，但还是体会到了逐渐提高模型准确率的满足感和成就感，这也激励着我进行进一步更深入地学习和探索。
+ 对如何调节参数有了自己的心得体会，一开始只会盲目调试，效果不好且难以对效果变化原因进行针对性地分析，后来深入地分析参数含义，有针对性地调节参数，大大提高了调参效率。

### 改进方案

+ 曾经尝试使用基于词的二元模型，使用Jieba进行分词操作，但反而会使准确率大大下降，我个人认为可能是因为数据集不够大造成的，词与词的共现频率太低，导致条件概率矩阵十分稀疏，语义相关的词与语义不太相关的词在条件概率矩阵上的数值相差不大，很容易在计算句子概率时被其他因素遮盖掉这种微小的差异。下一步可能使用爬虫获取更大的数据集进行训练，观察准确率的变化。
+ 对于基于字的四元模型，想办法克服硬件上的困难，尝试使用更大内存的电脑，或者使用其他的语言，解决字典在运行时占用空间过大的问题，使得模型能在更大的语料集上进行训练，以得到更好的效果。